{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ahead-hughes",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interior-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import datetime as dt\n",
    "import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys, os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../src/'))\n",
    "from model.vgg19 import VGG19\n",
    "from model.ViT import VisionTransformer\n",
    "from model.augmentation import aug_process\n",
    "from dataloader import dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-confusion",
   "metadata": {},
   "source": [
    "# mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "victorian-attachment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0\n",
      "WARNING:tensorflow:From /home/a_imagawa/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-durham",
   "metadata": {},
   "source": [
    "# model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "systematic-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'EfficientNet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-blocking",
   "metadata": {},
   "source": [
    "# file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lucky-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_open = open('../config.json', 'r')\n",
    "config = json.load(json_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "other-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = f'{config[\"filepath\"][\"dataset\"]}'\n",
    "partitions_path = f'{config[\"filepath\"][\"partitions\"]}'\n",
    "output_path = f'{config[\"filepath\"][\"output\"]}'\n",
    "file_name = f\"{model_type}_{config['data']['img_size']}_custom_loop\"\n",
    "log_dir = f\"{output_path}/logs/{file_name}\"\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eight-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt_path_list = glob.glob(f\"{partitions_path}/Test*\")\n",
    "train_txt_path_list = glob.glob(f\"{partitions_path}/Train*\")\n",
    "class_name_path = f\"{partitions_path}/ClassName.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-convention",
   "metadata": {},
   "source": [
    "# label to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "following-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(class_name_path, 'r')\n",
    "label_name_list = f.readlines()\n",
    "label_name_list = list(map(lambda tmp_path: tmp_path[:-1].split('/', 2)[2], label_name_list))\n",
    "f.close()\n",
    "label_to_index = dict((name, index) for index, name in enumerate(label_name_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-coast",
   "metadata": {},
   "source": [
    "# text to path list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cellular-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_path(txt_path_list):\n",
    "    path_list = []\n",
    "    for path in txt_path_list:\n",
    "        f = open(path, 'r')\n",
    "        path_list+=f.readlines()\n",
    "        f.close()\n",
    "    \n",
    "    path_list = list(map(lambda tmp_path: dataset_path+tmp_path[:-1], path_list))\n",
    "    path_list = sorted(list(set(path_list)-set(config[\"data\"][\"exclude_list\"])))\n",
    "    label_list = list(map(lambda tmp_path: label_to_index[tmp_path.split('/', 6)[6].rsplit('/', 1)[0]], path_list))\n",
    "    return path_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "american-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_list, label_list = txt_to_path(train_txt_path_list)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bacterial-worship",
   "metadata": {},
   "source": [
    "import imageio\n",
    "from joblib import Parallel, delayed\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "def check(path):\n",
    "    try:\n",
    "        imageio.imwrite(path, imageio.imread(path)[..., :3])\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "exclude_list = Parallel(n_jobs=-1)([delayed(check)(path) for path in tqdm.tqdm(img_path_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "funded-notification",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path_list, val_img_path_list,\\\n",
    "train_label_list, val_label_list = train_test_split(img_path_list, label_list,\n",
    "                                                    test_size=0.2, random_state=0)\n",
    "test_img_path_list, test_label_list = txt_to_path(test_txt_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "increasing-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dataloader = dataloader(config['data']['batch_size'], config['data']['img_size'],)\n",
    "train_ds = tmp_dataloader(train_img_path_list, train_label_list, shuffle_buffer=100)\n",
    "val_ds = tmp_dataloader(val_img_path_list, val_label_list, is_train=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-raleigh",
   "metadata": {},
   "source": [
    "# def model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "constant-papua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "num_classes = len(label_to_index)+1\n",
    "with strategy.scope():\n",
    "    if model_type=='vgg19':\n",
    "        model = VGG19(num_classes, img_size=config['data']['img_size'],)\n",
    "    if model_type=='ViT':\n",
    "        model = VisionTransformer(num_classes=num_classes, img_size=config['data']['img_size'])\n",
    "    if model_type=='EfficientNet':\n",
    "        model = tf.keras.applications.EfficientNetB3(classes=num_classes, weights=None,\n",
    "                                                     input_shape=(config['data']['img_size'], config['data']['img_size'], 3))\n",
    "    aug_model = aug_process(config['data']['img_size'])\n",
    "    \n",
    "    loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    def calc_loss(target_y, predicted_y):\n",
    "        return tf.math.reduce_mean(loss_obj(target_y, predicted_y))\n",
    "    \n",
    "    acc_func = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    def calc_acc(target_y, predicted_y):\n",
    "        return tf.math.reduce_mean(acc_func(target_y, predicted_y))\n",
    "    \n",
    "optimizer = tfa.optimizers.RectifiedAdam(lr=config['data']['lr'], clipnorm=0.01)\n",
    "optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-budget",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "negative-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch like learning rate scheduler\n",
    "class ReduceLROnPlateau():\n",
    "    def __init__(self, optimizer, patience, factor):\n",
    "        self.optimizer = optimizer\n",
    "        self.patience = patience\n",
    "        self.factor = factor\n",
    "        self.best_loss = None\n",
    "        self.count = 0\n",
    "    def step(self, loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = loss\n",
    "        elif self.best_loss>loss:\n",
    "            self.count = 0\n",
    "            self.best_loss = loss\n",
    "        else:\n",
    "            self.count+=1\n",
    "            if self.count==self.patience:\n",
    "                self.optimizer.learning_rate = self.optimizer.learning_rate*self.factor\n",
    "                self.count=0\n",
    "\n",
    "@tf.function\n",
    "def train_step(input_img, label, optimizer):\n",
    "    with tf.GradientTape() as GT:\n",
    "        aug_img = aug_model(input_img)\n",
    "        prediction = model(aug_img, training=True)\n",
    "        loss = calc_loss(label, prediction)\n",
    "        acc = calc_acc(label, prediction)\n",
    "        scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "    scaled_grad = GT.gradient(scaled_loss, model.trainable_variables)\n",
    "    grad = optimizer.get_unscaled_gradients(scaled_grad)\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
    "    return loss, acc\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(input_img, label, optimizer):\n",
    "    per_replica_losses, per_replica_acc = strategy.run(train_step, args=(input_img, label, optimizer))\n",
    "    loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_losses, axis=None)\n",
    "    acc = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_acc, axis=None)\n",
    "    return loss, acc\n",
    "\n",
    "@tf.function\n",
    "def val_step(input_img, label):\n",
    "    prediction = model(input_img)\n",
    "    loss = calc_loss(label, prediction)\n",
    "    acc = calc_acc(label, prediction)\n",
    "    return loss, acc\n",
    "\n",
    "@tf.function\n",
    "def distributed_val_step(input_img, label):\n",
    "    per_replica_losses, per_replica_acc = strategy.run(val_step, args=(input_img, label))\n",
    "    loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_losses, axis=None)\n",
    "    acc = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_acc, axis=None)\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "scenic-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tf.summary.create_file_writer(f\"{log_dir}/train\")\n",
    "val_writer = tf.summary.create_file_writer(f\"{log_dir}/validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "alternative-poultry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 340 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 340 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 340 all-reduces with algorithm = nccl, num_packs = 1\n",
      "epoch:1 - train_loss:5.8437500 - val_loss:6.2773438 - time:0:13:55 - leaning_rate:0.00020000\n",
      "epoch:2 - train_loss:5.3359375 - val_loss:5.3906250 - time:0:09:16 - leaning_rate:0.00020000\n",
      "epoch:3 - train_loss:4.9687500 - val_loss:4.8984375 - time:0:09:16 - leaning_rate:0.00020000\n",
      "epoch:4 - train_loss:4.7187500 - val_loss:4.5195312 - time:0:09:15 - leaning_rate:0.00020000\n",
      "epoch:5 - train_loss:4.5117188 - val_loss:4.3750000 - time:0:09:18 - leaning_rate:0.00020000\n",
      "epoch:6 - train_loss:4.3476562 - val_loss:4.2460938 - time:0:09:19 - leaning_rate:0.00020000\n",
      "epoch:7 - train_loss:4.1992188 - val_loss:4.0078125 - time:0:09:19 - leaning_rate:0.00020000\n",
      "epoch:8 - train_loss:4.0703125 - val_loss:3.8886719 - time:0:09:18 - leaning_rate:0.00020000\n",
      "CPU times: user 3h 57min 6s, sys: 17min 32s, total: 4h 14min 39s\n",
      "Wall time: 1h 19min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, patience=2, factor=0.95)\n",
    "\n",
    "for epoch in range(config['data']['epochs']):\n",
    "    \n",
    "    st = dt.datetime.now()\n",
    "    #train\n",
    "    tmp_loss_list = []\n",
    "    tmp_acc_list = []\n",
    "    for inputs, outputs in train_ds:\n",
    "        tmp_loss, tmp_acc = distributed_train_step(inputs, outputs, optimizer)\n",
    "        tmp_loss_list.append(tmp_loss)\n",
    "        tmp_acc_list.append(tmp_acc)\n",
    "        \n",
    "    train_loss = tf.math.reduce_mean(tmp_loss_list).numpy()\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    train_acc = tf.math.reduce_mean(tmp_acc_list).numpy()\n",
    "    train_acc_list.append(train_acc)\n",
    "    \n",
    "    #validation\n",
    "    tmp_loss_list = []\n",
    "    tmp_acc_list = []\n",
    "    for inputs, outputs in val_ds:\n",
    "        tmp_loss, tmp_acc = distributed_val_step(inputs, outputs)\n",
    "        tmp_loss_list.append(tmp_loss)\n",
    "        tmp_acc_list.append(tmp_acc)\n",
    "    \n",
    "    #exclude under or over flow\n",
    "    tmp_loss_list = list(np.array(tmp_loss_list)[np.bool_(1-tf.math.is_nan(tmp_loss_list).numpy())])\n",
    "    val_loss = tf.math.reduce_mean(tmp_loss_list).numpy()\n",
    "    val_loss_list.append(val_loss)\n",
    "    \n",
    "    val_acc = tf.math.reduce_mean(tmp_acc_list).numpy()\n",
    "    val_acc_list.append(val_acc)\n",
    "    \n",
    "    tmp_time = dt.datetime.now()-st\n",
    "    str_time = str(tmp_time).split('.')[0]\n",
    "    learning_rate = optimizer.learning_rate.numpy()\n",
    "    \n",
    "    for writer, loss, acc in zip([train_writer, val_writer], \n",
    "                                 [train_loss, val_loss],\n",
    "                                 [train_acc, val_acc]):\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar(\"epoch_loss\", loss, step=epoch)\n",
    "            tf.summary.scalar(\"epoch_acc\", acc, step=epoch)\n",
    "            writer.flush()\n",
    "    print(f'epoch:{epoch+1} - train_loss:{train_loss:.7f} - val_loss:{val_loss:.7f} - time:{str_time} - leaning_rate:{learning_rate:.8f}')\n",
    "    model.save_weights(f\"{output_path}/{model_type}.h5\")\n",
    "    lr_scheduler.step(val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
